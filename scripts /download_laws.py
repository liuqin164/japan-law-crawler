#!/usr/bin/env python3
import argparse
import json
import re
import time
import urllib.request
import urllib.parse
from pathlib import Path

def fetch_json(url, timeout=30):
    """å¸¦è¶…æ—¶ä¿æŠ¤çš„è¯·æ±‚å‡½æ•°"""
    req = urllib.request.Request(url, headers={'User-Agent': 'Mozilla/5.0 (OpenClaw-Crawler)'})
    with urllib.request.urlopen(req, timeout=timeout) as response:
        return json.loads(response.read().decode("utf-8"))

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--output-dir", required=True)
    parser.add_argument("--category-cd", default="013")
    parser.add_argument("--limit", type=int, default=None, help="æœ€å¤šä¸‹è½½å¤šå°‘éƒ¨æ³•ä»¤ååœæ­¢")
    args = parser.parse_args()

    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    offset = 1
    limit_per_page = 100
    active_count = 0
    target_cat = args.category_cd.zfill(3)

    print(f"ğŸš€ å¯åŠ¨æµå¼ä¸‹è½½ä»»åŠ¡ã€‚ç›®æ ‡åˆ†ç±»: {target_cat}", flush=True)

    while True:
        # é˜¶æ®µ 1: è·å–ä¸€é¡µç´¢å¼• (100æ¡)
        list_url = f"https://laws.e-gov.go.jp/api/2/laws?response_format=json&offset={offset}&limit={limit_per_page}"
        try:
            print(f"ğŸ“¡ æ­£åœ¨æ‰«æç´¢å¼•åç§»é‡: {offset}...", flush=True)
            data = fetch_json(list_url)
            laws = data.get("laws_response", {}).get("law_info_list", [])
            
            if not laws:
                print("ğŸ å·²åˆ°è¾¾ç´¢å¼•æœ«å°¾ã€‚")
                break

            # é˜¶æ®µ 2: ç«‹å³å¤„ç†è¿™ä¸€é¡µä¸­çš„æ¯ä¸€æ¡æ³•å¾‹ (æµå¼å¤„ç†)
            for law in laws:
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç”¨æˆ·è®¾å®šçš„ limit
                if args.limit and active_count >= args.limit:
                    print(f"ğŸ›‘ å·²è¾¾åˆ°è®¾å®šçš„ä¸‹è½½ä¸Šé™ ({args.limit})ï¼Œåœæ­¢ä»»åŠ¡ã€‚")
                    return

                law_id = law.get("law_id")
                law_name = law.get("law_name")

                try:
                    # ä¸‹è½½è¯¦æƒ…è¿›è¡Œç²¾å‡†è¿‡æ»¤
                    detail_url = f"https://laws.e-gov.go.jp/api/2/law_data/{law_id}?response_format=json&law_full_text_format=json&extraction_target=all"
                    detail_payload = fetch_json(detail_url)
                    
                    data_root = detail_payload.get("law_data_response", {})
                    revision_info = data_root.get("revision_info", {})
                    
                    # æ ¡éªŒåˆ†ç±»: å¿…é¡»æ˜¯ 013 (å›½ç¨)
                    current_cat = str(revision_info.get("category_cd", "")).zfill(3)
                    if current_cat != target_cat:
                        continue

                    # æ ¡éªŒçŠ¶æ€: å¿…é¡»æ˜¯ç°è¡Œ (éåºŸæ­¢)
                    repeal = revision_info.get("repeal_status")
                    if repeal in ["Repeal", "Expire", "LossOfEffectiveness"]:
                        continue

                    # æ‰§è¡Œä¿å­˜ (å³æ—¶è½ç›˜)
                    safe_name = re.sub(r"[^\w\-]", "_", law_name)
                    file_path = output_dir / f"{law_id}_{safe_name[:50]}.json"
                    file_path.write_text(json.dumps(detail_payload, ensure_ascii=False, indent=2))
                    
                    active_count += 1
                    print(f"   âœ… [{active_count}] å·²ä¿å­˜: {law_name}", flush=True)
                    
                    # è¯¦æƒ…ä¸‹è½½é—´éš”ï¼Œä¿æŠ¤API
                    time.sleep(0.3)

                except Exception as e:
                    # è¯¦æƒ…ä¸‹è½½å¤±è´¥åªè·³è¿‡å½“å‰æ¡ç›®ï¼Œä¸ä¸­æ–­å…¨é‡ä»»åŠ¡
                    continue

            # ç¿»é¡µé€»è¾‘
            if len(laws) < limit_per_page:
                break
            offset += limit_per_page
            
        except Exception as e:
            print(f"âŒ ç´¢å¼•è·å–å¼‚å¸¸ (Offset {offset}): {e}")
            # å¦‚æœç´¢å¼•è·å–å¤±è´¥ï¼Œå°è¯•è·³è¿‡è¿™é¡µç»§ç»­
            offset += limit_per_page
            time.sleep(2)

    print(f"\nğŸš€ ä»»åŠ¡å®Œæˆ! å…±ä¿å­˜ {active_count} éƒ¨æœ‰æ•ˆå›½ç¨æ³•ä»¤ã€‚")

if __name__ == "__main__":
    main()
